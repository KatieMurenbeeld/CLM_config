<?xml version="1.0"?>
<config_machines version="2.0">
<!-- This is an ordered list, not all fields are required, optional fields are noted below -->
<!-- MACH is the name that you will use in machine options -->
<machine MACH ="r2">
		
		<!-- DESC: a text description of the machine -->
		<DESC>Boise State HPC</DESC>
		
		<!-- NODENAME_REGEX: a regular expression used to identify this machine
		     		it must work on compute nodes as wellas login nodes, use machine option 
		to create_test or creat_newcase is this flag is not available -->
		<NODENAME_REGEX>r2.boisestate.edu</NODENAME_REGEX>
		
		<!-- OS: the operation system of this machine. Passed to cppflags for
		     		compiled programs as -DVALUE recognized are LINUX, AIX, Darwin, CNL -->
		<OS>LINUX</OS>
		
		<!-- PROXY: optional http proxy for access to the internet -->
		<PROXY>https://howto.get.out</PROXY>
		
		<!-- COMPILERS: compilers supported on this machine, comma separate list, first is default -->
		<COMPILERS>intel,gnu</COMPILERS>
		
		<!-- MPILIBS: mpiloibs supported on this machine, comma separated list, 
		     		first is default, mpi-serial is assumed and not required in this list -->
		<MPILIBS>mpi</MPILIBS>
		
		<!-- PROJECT: optional, a project or account number used for batch jobs
		     		can be overridden in environment or $HOME/.cime/config -->
		<PROJECT>couldbethis</PROJECT>
		
		<!-- CHARGE_ACCOUNT: optional, a project or account number used for batch jobs.
		     		This is the actual project used for cost accounting set in
		the batch script. Will default to PROJECT if not set. Can be overridden in 
		environment or $HOME/.cime/config -->
		<CHARGE_ACCOUNT>couldbethat</CHARGE_ACCOUNT>
		
		<!-- CIME_OUTPUT_ROOT: Base directory for case output,
		     		the case/bld and case/run directories are written below here -->
		<CIME_OUTPUT_ROOT>/home/kmurenbeeld/cesm/scratch</CIME_OUTPUT_ROOT>
		
		<!-- DIN_LOC_ROOT: location of the inputdata data directory
		     		inputdata is downloaded automatically on a case by case basis as
		long as the user has write access to this directory. We recommend that 
		all cime users on a system share an inputdata directory 
		as it can be quite large -->
		<DIN_LOC_ROOT>/scratch/leaf/share/CESM/inputdata</DIN_LOC_ROOT>
		
		<!-- DIN_LOC_ROOT_CLMFORC: override of DIN_LOC_ROOT specific to CLM
		     		forcing data -->
		<DIN_LOC_ROOT_CLMFORC>/scratch/leaf/share/CESM/inputdata/atm/wrf</DIN_LOC_ROOT_CLMFORC>
		
		<!-- DOUT_S_ROOT: root directory of short term archive files, short term
		     		archiving moves model output data out of the run directory, but
		keeps it on disk -->
		<DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
		
		<!-- BASELINE_ROOT: Root directory for system test baseline files -->
		<BASELINE_ROOT>/home/kmurenbeeld/cesm/cesm_baseline</BASELINE_ROOT>
		
		<!-- CCSM_CPRNC: location of the cprnc tool, compared model output in testing. -->
		<CCSM_CPRNC>/home/kmurenbeeld/clm5.0/cime/tools/cprnc/cprnc.F90</CCSM_CPRNC>
		
		<!-- GMAKE: gnu compatical make tool, default is 'gmake' -->
		<GMAKE>gmake</GMAKE>
		
		<!-- GMAKE_J: optional, number of thread to pass to the gmake flag -->
		<GMAKE_J>8</GMAKE_J>
		
		<!-- BATCH_SYSTEM: batch system used on this machine,
		     		supported values are: none, cobalt, lsf, pbs, slurm -->
		<BATCH_SYSTEM>slurm</BATCH_SYSTEM>
		
		<!-- SUPPORTED_BY: contact information for support for this system 
		     		this field is not used in code -->
		<SUPPORTED_BY>cseg</SUPPORTED_BY>
		
		<!-- MAX_TASKS_PER_NODE: maximum number of threads, tasks per shared 
		     		memory node, on this machine, should always be >= MAX_MPITASKS_PER_NODE -->
		<MAX_TASKS_PER_NODE>28</MAX_TASKS_PER_NODE>
		
		<!-- MAX_MPITASKS_PER_NODE: number of physical PES per shared node on
		     		this machine, in practice the MPI tasks per node will not exceed this value -->
		<MAX_MPITASKS_PER_NODE>28</MAX_MPITASKS_PER_NODE>
		
		<!-- PROJECT_REQUIRED: Does this machine require a project to be specified to
		     		the batch system? See PROJECT above -->
		<PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
		
		<!-- mpirun: The mpi exec to start a job on this machine, supported values 
		     		are values listed in MPILIBS above, default and mpi-serial -->
		<mpirun mpilib="default">
			<!-- name of the executable used to launch mpi jobs -->
			<executable>mpirun</executable>
			<arguments>
				<arg name="ntasks">-np{{total_tasks}}</arg>
				<arg name="labelstdout">-p "%g:"</arg>
				<arg name="threadplacement"> omplace </arg>
			</arguments>
		</mpirun>
		
		<mpirun mpilib="mpi-serial">
			<executable></executable>
		</mpirun>
		
		<module_system type="none">
			<!--
			<init_path lang="perl">/usr/bin/perl</init_path>
			<init_path lang="python">/usr/bin/python</init_path>
			<init_path lang="csh">/usr/bin/csh</init_path>
			<init_path lang="sh">/usr/bin/sh</init_path>
			<cmd_path lang="perl">module perl</cmd_path>
			<cmd_path lang="python">module python</cmd_path>
			<cmd_path lang="csh">module</cmd_path>
			<cmd_path lang="sh">module</cmd_path>
			

			<modules>
				<command name="purge"/>
				<command name="load">shared</command>
			</modules>
			
			<modules compiler="intel">
				<command name="load">intel/compiler/64/2018/18.0.</command>
				<command name="load">intel/mkl/64/2018/3,222</command>
			</modules>
			
			<modules compiler="gnu">
				<command name="load">gcc/6.4.0</command>
			</modules> -->
		</module_system>
		
		<!-- environment variables, a blank entry will unset a variable -->
		<environment_variables>
			<env name="OMP_STACKSIZE">256M</env>
			<env name="MPI_TYPE_DEPTH">16</env>
		</environment_variables>
		
	</machine>	
</config_machines>
